{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow-time series prediction.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOsXfb+ndr1ukg+bgFFylRq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/renxiaowei/saved_code_analysis/blob/main/TensorFlow_time_series_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMQ_03cVDRE_"
      },
      "source": [
        "This documents are for the tensorflow-based code structure.\n",
        "The original code is from 'https://github.com/jsyoon0823/Time-series-prediction'\n",
        "The main target is to predict based on the sequence variation.\n",
        "The code is released based on the python commond, not jupyter.\n",
        "\n",
        "Here, I illustrate it based on the Jupyter format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liImto8OEpM8"
      },
      "source": [
        "The first step is the runing of the code.\n",
        "原先的仅仅是一条python command：\n",
        "\n",
        "!python3 main_time_series_prediction.py --train_rate 1 --seq_len 7 --task regression --model_type lstm --h_dim 10 --n_layer 3 --batch_size 2 --epoch 2 --learning_rate 0.01 --metric_name mae\n",
        "\n",
        "但是这里我们把main_time_series_prediction.py进行了拆分，在jupyter。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXrf6UbnEbr3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17bb8073-109e-4301-e06d-fee7130d1c8f"
      },
      "source": [
        "!git clone https://github.com/jsyoon0823/Time-series-prediction.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Time-series-prediction'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 39 (delta 0), reused 3 (delta 0), pack-reused 36\u001b[K\n",
            "Unpacking objects: 100% (39/39), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSpqOJMbEs8n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d76ad71a-893d-4153-d546-192e513e60f7"
      },
      "source": [
        "!ls\n",
        "%cd Time-series-prediction/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  Time-series-prediction\n",
            "/content/Time-series-prediction\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKNqx3hW1dwg"
      },
      "source": [
        "# 加载主函数中必要的package\n",
        "# Necessary packages\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from data_loader import data_loader\n",
        "from basic_rnn_lstm_gru import GeneralRNN\n",
        "from basic_attention import Attention\n",
        "from utils import performance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlovYwVK2XEr"
      },
      "source": [
        "# 定义主函数，包括：\n",
        "# 加载training 和 testing dataset\n",
        "# 加载预定义的参数值\n",
        "# 定义model\n",
        "# 做training\n",
        "# 做testing\n",
        "# 做性能计算\n",
        "\n",
        "def main (args):  \n",
        "  \"\"\"Time-series prediction main function.\n",
        "  \n",
        "  Args:\n",
        "    - train_rate: training data ratio\n",
        "    - seq_len: sequence length\n",
        "    - task: classification or regression\n",
        "    - model_type: rnn, lstm, gru, or attention\n",
        "    - h_dim: hidden state dimensions\n",
        "    - n_layer: number of layers\n",
        "    - batch_size: the number of samples in each mini-batch\n",
        "    - epoch: the number of iterations\n",
        "    - learning_rate: learning rates\n",
        "    - metric_name: mse or mae\n",
        "  \"\"\"\n",
        "  # Load data\n",
        "  train_x, train_y, test_x, test_y = data_loader(args.train_rate, \n",
        "                                                 args.seq_len)\n",
        "  \n",
        "  # Model traininig / testing\n",
        "  model_parameters = {'task': args.task,\n",
        "                      'model_type': args.model_type,\n",
        "                      'h_dim': args.h_dim,\n",
        "                      'n_layer': args.n_layer,\n",
        "                      'batch_size': args.batch_size,\n",
        "                      'epoch': args.epoch,\n",
        "                      'learning_rate': args.learning_rate}\n",
        "  \n",
        "  if args.model_type in ['rnn','lstm','gru']:\n",
        "    general_rnn = GeneralRNN(model_parameters)  # 类的对象化  \n",
        "    general_rnn.fit(train_x, train_y)\n",
        "    test_y_hat = general_rnn.predict(test_x)\n",
        "  elif args.model_type == 'attention':\n",
        "    basic_attention = Attention(model_parameters)    \n",
        "    basic_attention.fit(train_x, train_y)\n",
        "    test_y_hat = basic_attention.predict(test_x)\n",
        "  \n",
        "  # Evaluation\n",
        "  result = performance(test_y, test_y_hat, args.metric_name)\n",
        "  print('Performance (' + args.metric_name + '): ' + str(result))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJwXjDVJ8pU4"
      },
      "source": [
        "# 因为加载的参数比较多而杂，为了便于管理，定义一个类Para\n",
        "# 基于这个类，可以简化参数结构。\n",
        "# 同时这个类，实际上是跟source code中的，parser = argparse.ArgumentParser()，相匹配的。\n",
        "class Para:\n",
        "  train_rate = 0.8\n",
        "  seq_len = 7\n",
        "  task = 'regression'\n",
        "  model_type = 'lstm'\n",
        "  h_dim = 10\n",
        "  n_layer = 3\n",
        "  batch_size = 2\n",
        "  epoch = 2\n",
        "  learning_rate = 0.01\n",
        "  metric_name = 'mae'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hed5aZrG2cYD"
      },
      "source": [
        "# 对参数进行赋值\n",
        "# 运行主函数\n",
        "args = Para()\n",
        "\n",
        "args.train_rate = 0.8\n",
        "args.seq_len = 7\n",
        "\n",
        "args.task = 'regression'\n",
        "args.model_type = 'lstm'\n",
        "args.h_dim = 10\n",
        "args.n_layer = 3\n",
        "args.batch_size = 64\n",
        "args.epoch = 100\n",
        "args.learning_rate = 0.01\n",
        "args.metric_name = 'mae'\n",
        "\n",
        "# Call main function  \n",
        "main(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdCz9Y7yIBG9"
      },
      "source": [
        "# 备份的code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKp4AGLs2Ivw"
      },
      "source": [
        "# 这个是source code中的关于参数传入的结构，主要是基于parser = argparse.ArgumentParser()\n",
        "# 如果我们想要添加或者删减一些参数，可以添加parser.add_argument(**）的模块就可以\n",
        "# 基于这个结构，我们就可以直接一条命令，就可以直接运行函数。\n",
        "##  \n",
        "if __name__ == '__main__':\n",
        "  \n",
        "  # Inputs for the main function\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument(\n",
        "      '--train_rate',\n",
        "      help='training data ratio',\n",
        "      default=0.8,\n",
        "      type=str)\n",
        "  parser.add_argument(\n",
        "      '--seq_len',\n",
        "      help='sequence length',\n",
        "      default=7,\n",
        "      type=int)\n",
        "  parser.add_argument(\n",
        "      '--model_type',\n",
        "      choices=['rnn','gru','lstm','attention'],\n",
        "      default='attention',\n",
        "      type=str)\n",
        "  parser.add_argument(\n",
        "      '--h_dim',\n",
        "      default=10,\n",
        "      type=int)\n",
        "  parser.add_argument(\n",
        "      '--n_layer',\n",
        "      default=3,\n",
        "      type=int)\n",
        "  parser.add_argument(\n",
        "      '--batch_size',\n",
        "      default=32,\n",
        "      type=int)\n",
        "  parser.add_argument(\n",
        "      '--epoch',\n",
        "      default=100,\n",
        "      type=int)\n",
        "  parser.add_argument(\n",
        "      '--learning_rate',\n",
        "      default=0.01,\n",
        "      type=float)\n",
        "  parser.add_argument(\n",
        "      '--task',\n",
        "      choices=['classification','regression'],\n",
        "      default='regression',\n",
        "      type=str)\n",
        "  parser.add_argument(\n",
        "      '--metric_name',\n",
        "      choices=['mse','mae'],\n",
        "      default='mae',\n",
        "      type=str)\n",
        "  \n",
        "  args = parser.parse_args() \n",
        "\n",
        "  # Call main function  \n",
        "  main(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dubC3_9XIj5E"
      },
      "source": [
        "# 这个就是python based command\n",
        "!python3 main_time_series_prediction.py --train_rate 0.8 --seq_len 7 --task regression --model_type lstm --h_dim 10 --n_layer 3 --batch_size 32 --epoch 100 --learning_rate 0.01 --metric_name mae"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0lJHnyzOEXq"
      },
      "source": [
        "# 代码分析"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSa4AbWAOJBp"
      },
      "source": [
        "# 对应着数据集加载，data_loader.py\n",
        "# Necessary Packages\n",
        "import numpy as np\n",
        "from utils import MinMaxScaler\n",
        "\n",
        "\n",
        "def data_loader(train_rate = 0.8, seq_len = 7):\n",
        "  \"\"\"Loads Google stock data.\n",
        "  \n",
        "  Args:\n",
        "    - train_rate: the ratio between training and testing sets\n",
        "    - seq_len: sequence length\n",
        "    \n",
        "  Returns:\n",
        "    - train_x: training feature\n",
        "    - train_y: training labels\n",
        "    - test_x: testing features\n",
        "    - test_y: testing labels\n",
        "  \"\"\"\n",
        "  \n",
        "  # Load data\n",
        "  ori_data = np.loadtxt('data/google.csv', delimiter=',', skiprows = 1)\n",
        "  # 这个是一次性把所有的数据集都load进来。如果数据集size比较大怎么办。\n",
        "  # 可能方法还是分批次的读取，比如说：在一次epoch中，overall 的数据集随机分成4份，每一份load进来训练，然后再load下一份。\n",
        "  # 这样就实现了，数据频繁的load和数据集过大之间的平衡。随机分成的份数，是一个参数，如果参数过大，就是频繁的load很耽误时间；如果参数过小，那么数据size就太大，load不进来。\n",
        "  # 这个参数就可以定义为，memory最大支撑的数据size小，分割数据集的分数。\n",
        "  # 这里数据集，从CSV加载成numpy\n",
        "\n",
        "  # 另外一个问题，读取的数据最后是一个大文件，而不是小文件，频繁的读取太耽误时间。\n",
        "\n",
        "  # Reverse the time order\n",
        "  reverse_data = ori_data[::-1]\n",
        "  # 为啥reverse，这个可能是日K线，然后本身数据集是倒序排列的（也就是最新日期的在最上边），所以要倒过来。\n",
        "  # Normalization\n",
        "  norm_data = MinMaxScaler(reverse_data) #应该是归一化，应该是[0,1]\n",
        "    \n",
        "  # Build dataset\n",
        "  data_x = []\n",
        "  data_y = []\n",
        "  \n",
        "\n",
        "\n",
        "  for i in range(0, len(norm_data[:,0]) - seq_len): \n",
        "    # 这个就是sliding window，一个sample，一个sample，去shift，然后去预测下一个。这里是以starting point i为基准，所以，末尾减去。\n",
        "    # Previous seq_len data as features\n",
        "    temp_x = norm_data[i:i + seq_len,:] # size is 7 *5\n",
        "    # Values at next time point as labels\n",
        "    temp_y = norm_data[i + seq_len, [-1]] # label 仅仅是最后一个维度的，所以这个 1*1的数据\n",
        "    data_x = data_x + [temp_x]\n",
        "    data_y = data_y + [temp_y]\n",
        "    \n",
        "  data_x = np.asarray(data_x)\n",
        "  data_y = np.asarray(data_y)\n",
        "            \n",
        "  # Train / test Division   \n",
        "  idx = np.random.permutation(len(data_x)) \n",
        "  # 数据集整理好之后，就是打乱，然后做split。permutation就是做shuffle的作用。\n",
        "  # shuffle的方法是，打乱数据的index，然后基于index做提取。\n",
        "  train_idx = idx[:int(train_rate * len(data_x))] # shuffle后，0到比例之前的做training，后边的做test\n",
        "  test_idx = idx[int(train_rate * len(data_x)):]\n",
        "        \n",
        "  train_x, test_x = data_x[train_idx, :, :], data_x[test_idx, :, :]\n",
        "  train_y, test_y = data_y[train_idx, :], data_y[test_idx, :]\n",
        "    \n",
        "  return train_x, train_y, test_x, test_y\n",
        "\n",
        "  # 这个函数就是一个数据提取，shuffle，提取的过程。\n",
        "  # 主要是用来，做training + validation 和 test之间的split和shuffle。\n",
        "  # 这个应该是epoch之外的操作，所以只需要一次就可以了，或者离线操作。\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDcBb0aCe_r6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWoPHLoxRBkR",
        "outputId": "f654efad-4a04-466f-c9b3-092761f0862a"
      },
      "source": [
        "  # 用来做实验验证\n",
        "  \n",
        "  import numpy as np\n",
        "  # Load data\n",
        "  ori_data = np.loadtxt('data/google.csv', delimiter=',', skiprows = 1)\n",
        "  print(ori_data[0,:])\n",
        "  # Reverse the time order\n",
        "  # 首先看python切片中双冒号的定义：list[<start>:<stop>:<step>]， 很显然这里就是倒序，而且只是第一个维度。\n",
        "  # 同样可以用多维数组中，b[:, ::-1]\n",
        "  reverse_data = ori_data[::-1]\n",
        "  print(reverse_data[0,:])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[8.28659973e+02 8.33450012e+02 8.28349976e+02 1.24770000e+06\n",
            " 8.31659973e+02]\n",
            "[  568.00257    568.00257    552.922516 13100.         558.462551]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_InxHT8dY-t_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UW3I5tTSY8Gn"
      },
      "source": [
        "# 代码分析"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIVz1VmFY_gO"
      },
      "source": [
        "#这个代码来自basic_rnn_lstm_gru.py\n",
        "#主要作用就是定义一个类，这个类里边，定义各种model\n",
        "# Necessary packages\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from tensorflow.keras import layers\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from utils import binary_cross_entropy_loss, mse_loss, rnn_sequential\n",
        "\n",
        "\n",
        "class GeneralRNN():\n",
        "  \"\"\"RNN predictive model to time-series.\n",
        "  \n",
        "  Attributes:\n",
        "    - model_parameters:\n",
        "      - task: classification or regression\n",
        "      - model_type: 'rnn', 'lstm', or 'gru'\n",
        "      - h_dim: hidden dimensions\n",
        "      - n_layer: the number of layers\n",
        "      - batch_size: the number of samples in each batch\n",
        "      - epoch: the number of iteration epochs\n",
        "      - learning_rate: the learning rate of model training\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, model_parameters):\n",
        "\n",
        "    #输入进来的各种参数\n",
        "    self.task = model_parameters['task']\n",
        "    self.model_type = model_parameters['model_type']\n",
        "    self.h_dim = model_parameters['h_dim']\n",
        "    self.n_layer = model_parameters['n_layer']\n",
        "    self.batch_size = model_parameters['batch_size']\n",
        "    self.epoch = model_parameters['epoch']\n",
        "    self.learning_rate = model_parameters['learning_rate']\n",
        "    \n",
        "    # 断定要求的model是否支持\n",
        "    assert self.model_type in ['rnn', 'lstm', 'gru']\n",
        "\n",
        "    # Predictor model define\n",
        "    # 初始化定义一个model。\n",
        "    self.predictor_model = None\n",
        "\n",
        "    # Set path for model saving\n",
        "    model_path = 'tmp'\n",
        "    if not os.path.exists(model_path):\n",
        "      os.makedirs(model_path)\n",
        "    self.save_file_name = '{}'.format(model_path) + \\\n",
        "                          datetime.now().strftime('%H%M%S') + '.hdf5'\n",
        "  \n",
        "\n",
        "  def _build_model(self, x, y):\n",
        "    \"\"\"Construct the model using feature and label statistics.\n",
        "    \n",
        "    Args:\n",
        "      - x: features\n",
        "      - y: labels\n",
        "      \n",
        "    Returns:\n",
        "      - model: predictor model\n",
        "    \"\"\"    \n",
        "    # Parameters\n",
        "    h_dim = self.h_dim\n",
        "    n_layer = self.n_layer\n",
        "    dim = len(x[0, 0, :])\n",
        "    max_seq_len = len(x[0, :, 0])\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Masking(mask_value=0., input_shape=(max_seq_len, dim)))\n",
        "\n",
        "    for _ in range(n_layer - 1):\n",
        "      model = rnn_sequential(model, self.model_type, h_dim, return_seq=True)\n",
        "\n",
        "    model = rnn_sequential(model, self.model_type, h_dim, \n",
        "                           return_seq=False)\n",
        "    adam = tf.keras.optimizers.Adam(learning_rate=self.learning_rate, \n",
        "                                    beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "\n",
        "    if self.task == 'classification':\n",
        "      model.add(layers.Dense(y.shape[-1], activation='sigmoid'))\n",
        "      model.compile(loss=binary_cross_entropy_loss, optimizer=adam)\n",
        "      \n",
        "    elif self.task == 'regression':\n",
        "      model.add(layers.Dense(y.shape[-1], activation='linear'))\n",
        "      model.compile(loss=mse_loss, optimizer=adam, metrics=['mse'])\n",
        "\n",
        "    return model\n",
        "  \n",
        "\n",
        "  def fit(self, x, y):\n",
        "    \"\"\"Fit the predictor model.\n",
        "    \n",
        "    Args:\n",
        "      - x: training features\n",
        "      - y: training labels\n",
        "      \n",
        "    Returns:\n",
        "      - self.predictor_model: trained predictor model\n",
        "    \"\"\"\n",
        "    #这个时候自动把training set，分割成validation 和 training\n",
        "    idx = np.random.permutation(len(x))\n",
        "    train_idx = idx[:int(len(idx)*0.8)]\n",
        "    valid_idx = idx[int(len(idx)*0.8):]\n",
        "    \n",
        "    train_x, train_y = x[train_idx], y[train_idx]\n",
        "    valid_x, valid_y = x[valid_idx], y[valid_idx]\n",
        "    \n",
        "    self.predictor_model = self._build_model(train_x, train_y)\n",
        "\n",
        "    # Callback for the best model saving\n",
        "    save_best = ModelCheckpoint(self.save_file_name, monitor='val_loss',\n",
        "                                mode='min', verbose=False,\n",
        "                                save_best_only=True)\n",
        "\n",
        "    # Train the model\n",
        "    self.predictor_model.fit(train_x, train_y, \n",
        "                             batch_size=self.batch_size, epochs=self.epoch, \n",
        "                             validation_data=(valid_x, valid_y), \n",
        "                             callbacks=[save_best], verbose=True)\n",
        "\n",
        "    self.predictor_model.load_weights(self.save_file_name)\n",
        "    os.remove(self.save_file_name)\n",
        "\n",
        "    return self.predictor_model\n",
        "  \n",
        "  \n",
        "  def predict(self, test_x):\n",
        "    \"\"\"Return the temporal and feature importance.\n",
        "    \n",
        "    Args:\n",
        "      - test_x: testing features\n",
        "      \n",
        "    Returns:\n",
        "      - test_y_hat: predictions on testing set\n",
        "    \"\"\"\n",
        "    test_y_hat = self.predictor_model.predict(test_x)\n",
        "    return test_y_hat\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}